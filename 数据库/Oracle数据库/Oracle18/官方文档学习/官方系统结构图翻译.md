![image-20201007224651289](../../../../../../AppData/Roaming/Typora/typora-user-images/image-20201007224651289.png)

​		Oracle数据库至少由一个数据库实例和一个数据库组成。数据库实例处理内存和进程。数据库由称为数据文件的物理文件组成，可以是非容器数据库，也可以是多租户容器数据库。Oracle数据库在操作期间还会使用几个数据库系统文件。

​		单实例数据库体系结构由一个数据库实例和一个数据库组成。数据库和数据库实例之间存在一对一的关系。可以在同一台服务器机器上安装多个单实例数据库。每个数据库都有单独的数据库实例。此配置对于在同一台机器上运行不同版本的Oracle数据库非常有用。

​		Oracle Real Application Clusters (Oracle RAC)数据库体系结构由运行在独立服务器机器上的多个实例组成。它们都共享同一个数据库。服务器机器集群在一端显示为单个服务器，在另一端显示终端用户和应用程序。这种配置是为高可用性、可伸缩性和高端性能而设计的。

​		侦听器是一个数据库服务器进程。它接收客户端请求，建立到数据库实例的连接，然后将客户端连接移交给服务器进程。侦听器可以在数据库服务器上本地运行，也可以远程运行。典型的Oracle RAC环境是远程运行的。

![image-20201007224716864](../../../../../../AppData/Roaming/Typora/typora-user-images/image-20201007224716864.png)

​		一个数据库实例包含一组Oracle数据库后台进程和内存结构。主要的内存结构是系统全局区(SGA)和程序全局区(PGAs)。后台进程对数据库中存储的数据(数据文件)进行操作，并使用内存结构完成它们的工作。数据库实例仅存在于内存中。

​		Oracle数据库还创建服务器进程来处理代表客户端程序到数据库的连接，并执行客户端程序的工作;例如，解析和运行SQL语句，以及检索结果并将结果返回给客户机程序。这些类型的服务器进程也称为前台进程。

有关更多信息，请参见Oracle数据库实例。

![image-20201007225802748](../../../../../../AppData/Roaming/Typora/typora-user-images/image-20201007225802748.png)

​		系统全局区域(SGA)是包含一个Oracle数据库实例的数据和控制信息的内存区域。所有服务器和后台进程共享SGA。当您启动一个数据库实例时，将显示为SGA分配的内存量。SGA包括以下数据结构:

- 共享池:缓存用户之间可以共享的各种结构;例如，共享池存储解析后的SQL、PL/SQL代码、系统参数和数据字典信息。数据库中发生的几乎所有操作都涉及到共享池。例如，如果用户执行一条SQL语句，则Oracle数据库将访问共享池。
- 闪回缓冲区:是SGA中的可选组件。当启用Flashback数据库时，启动名为Recovery Writer process (RVWR)的后台进程。RVWR定期将修改过的块从缓冲区缓存复制到闪回缓冲区，并按顺序将闪回数据库数据从闪回缓冲区写入到闪回数据库日志中，这些日志被循环重用。

- 数据库缓冲区缓存:存储从数据文件中读取的数据块的副本的内存区域。缓冲区是一个主内存地址，缓冲区管理器在其中临时缓存当前或最近使用的数据块。并发连接到数据库实例的所有用户共享对缓冲区缓存的访问。

- 数据库智能Flash缓存:是用于运行在Solaris或Oracle Linux上的数据库缓冲区缓存的可选内存扩展。它为数据库块提供了二级缓存。对于读密集型在线事务处理(OLTP)工作负载和数据仓库(DW)环境中的特别查询和批量数据修改，它可以提高响应时间和总体吞吐量。数据库智能闪存缓存驻留在一个或多个闪存磁盘设备上，这些设备是使用闪存的固态存储设备。数据库智能闪存缓存通常比附加的主存更经济，而且比磁盘驱动器快一个数量级。

- 重做日志缓冲区:是SGA中的一个循环缓冲区，保存有关数据库更改的信息。此信息存储在重做条目中。重做条目包含重构(或重做)通过数据操作语言(DML)、数据定义语言(DDL)或内部操作对数据库所做更改所需的信息。重做条目用于必要时的数据库恢复。

- 大池:是一个可选的内存区域，用于分配比共享池更大的内存。大池可以为共享服务器的用户全局区域(UGA)和Oracle XA接口(在事务与多个数据库交互时使用)、并行执行语句时使用的消息缓冲区和Recovery Manager I/O slave提供大的内存分配。

- in - memory Area:是一个可选组件，允许对象(表、分区和其他类型)以一种称为columnar格式的新格式存储在内存中。这种格式使扫描、连接和聚合的执行速度比传统的磁盘上格式快得多，从而为OLTP和DW环境提供了快速的报告和DML性能。这个特性对于操作几个列返回许多行的分析应用程序特别有用，而对于OLTP，它操作几个行返回许多列。

- 共享I/O池(SecureFiles):用于对SecureFile大对象(lob)进行大型I/O操作。lob是一组数据类型，设计用于保存大量数据。SecureFile是一个LOB存储参数，允许重复数据删除、加密和压缩。

- 流池:用于Oracle Streams、数据泵和GoldenGate集成的捕获和应用过程。流池存储缓冲的队列消息，并为Oracle流捕获进程和应用进程提供内存。除非您特别地配置它，流池的大小从0开始。在使用Oracle流时，池大小会根据需要动态增长。

- Java池:用于Java虚拟机(JVM)中所有特定于会话的Java代码和数据。Java池内存的使用方式不同，这取决于Oracle数据库运行的模式。

- 固定的SGA:是一个内部管理区域，包含关于数据库和数据库实例状态的一般信息，以及进程之间通信的信息。

![image-20201007230232716](../../../../../../AppData/Roaming/Typora/typora-user-images/image-20201007230232716.png)

程序全局区域(PGA)是一个非共享内存区域，它包含服务器和后台进程专用的数据和控制信息。Oracle数据库创建服务器进程来处理代表客户机程序到数据库的连接。在专用的服务器环境中，将为启动的每个服务器和后台进程创建一个PGA。每个PGA由堆栈空间和一个用户全局区域(UGA)组成。当使用PGA的关联服务器或后台进程终止时，PGA将被释放。



在共享服务器环境中，多个客户机用户共享服务器进程。UGA被移动到SGA(如果配置共享池或大池)，只留给PGA堆栈空间。



堆栈空间是分配给保存会话变量和数组的内存。



在一个专门的伺服器会话中，佐治亚大学由以下部分组成:



SQL工作区域:这些是用于内存密集型SQL操作的PGA内存的私有分配。排序区域由对数据进行排序的函数使用，例如order by和GROUP by。哈希区域用于执行表的哈希连接。位图合并区域用于合并从多个位图索引扫描中检索到的数据。

会话内存:这个用户会话数据存储区域被分配给会话变量，例如登录信息和数据库会话所需的其他信息。OLAP池管理OLAP数据页，它们相当于数据块。

私有SQL区域:该区域保存有关已解析SQL语句的信息，以及用于处理的其他特定于会话的信息。当服务器进程执行SQL或PL/SQL代码时，该进程使用私有SQL区域来存储绑定变量值、查询执行状态信息和查询执行工作区。相同或不同会话中的多个私有SQL区域可以指向SGA中的单个执行计划。持久区域包含绑定变量值。运行时区域包含查询执行状态信息。游标是私有SQL区域中特定区域的名称或句柄。您可以将游标看作客户端上的指针和服务器端上的状态。由于游标与私有SQL区域密切相关，所以这两个术语有时可以互换使用。

![image-20201007230431923](../../../../../../AppData/Roaming/Typora/typora-user-images/image-20201007230431923.png)

Background processes are part of the database instance and perform maintenance tasks required to operate the database and to maximize performance for multiple users. Each background process performs a unique task, but works with the other processes. Oracle Database creates background processes automatically when you start a database instance. The background processes that are present depend on the features that are being used in the database. When you start a database instance, mandatory background processes automatically start. You can start optional background processes later as required.

Mandatory background processes are present in all typical database configurations. These processes run by default in a read/write database instance started with a minimally configured initialization parameter file. A read-only database instance disables some of these processes. Mandatory background processes include the Process Monitor Process (PMON), Process Manager Process (PMAN), Listener Registration Process (LREG), System Monitor Process (SMON), Database Writer Process (DBWn), Checkpoint Process (CKPT), Manageability Monitor Process (MMON), Manageability Monitor Lite Process (MMNL), Recoverer Process (RECO), and Log Writer Process (LGWR).

Most optional background processes are specific to tasks or features. Some common optional processes include Archiver Processes (ARCn), Job Queue Coordinator Process (CJQ0), Recovery Writer Process (RVWR), Flashback Data Archive Process (FBDA), and Space Management Coordinator Process (SMCO).

Slave processes are background processes that perform work on behalf of other processes; for example, the Dispatcher Process (Dnnn) and Shared Server Process (Snnn).

![image-20201007230520977](../../../../../../AppData/Roaming/Typora/typora-user-images/image-20201007230520977.png)

The shared pool is a component of the System Global Area (SGA) and is responsible for caching various types of program data. For example, the shared pool stores parsed SQL, PL/SQL code, system parameters, and data dictionary information. The shared pool is involved in almost every operation that occurs in the database. For example, if a user executes a SQL statement, then Oracle Database accesses the shared pool.

The shared pool is divided into several subcomponents:

- **Library cache:** Is a shared pool memory structure that stores executable SQL and PL/SQL code. This cache contains the shared SQL and PL/SQL areas and control structures, such as locks and library cache handles. In a shared server architecture, the library cache also contains the User Global Area (UGA). When a SQL statement is executed, the database attempts to reuse previously executed code. If a parsed representation of a SQL statement exists in the library cache and can be shared, the database reuses the code. This action is known as a soft parse or a library cache hit. Otherwise, the database must build a new executable version of the application code, which is known as a hard parse or a library cache miss.
- **Reserved pool:** Is a memory area in the shared pool that Oracle Database can use to allocate large contiguous chunks of memory. The database allocates memory from the shared pool in chunks. Chunking allows large objects (over 5 KB) to be loaded into the cache without requiring a single contiguous area. In this way, the database reduces the possibility of running out of contiguous memory because of fragmentation.
- **Data dictionary cache:** Stores information about database objects (that is, dictionary data). This cache is also known as the row cache because it holds data as rows instead of buffers.
- **Server result cache:** Is a memory pool within the shared pool and holds result sets. The server result cache contains the SQL query result cache and PL/SQL function result cache, which share the same infrastructure. The SQL query result cache stores the results of queries and query fragments. Most applications benefit from this performance improvement. The PL/SQL function result cache stores function result sets. Good candidates for result caching are frequently invoked functions that depend on relatively static data.
- **Other components:** Include enqueues, latches, Information Lifecycle Management (ILM) bitmap tables, Active Session History (ASH) buffers, checkpoint queue, a least recently used (LRU) list, and other minor memory structures. Enqueues are shared memory structures (locks) that serialize access to database resources. They can be associated with a session or transaction. Examples are: Controlfile Transaction, Datafile, Instance Recovery, Media Recovery, Transaction Recovery, Job Queue, and so on. Latches are used as a low-level serialization control mechanism used to protect shared data structures in the SGA from simultaneous access. Examples are cache buffers LRU chain, row cache objects, library cache pin, and log file parallel write.

For more information, see [Shared Pool](http://www.oracle.com/pls/topic/lookup?ctx=db18&id=CNCPT1226).

![image-20201007230554672](../../../../../../AppData/Roaming/Typora/typora-user-images/image-20201007230554672.png)

The large pool is an optional memory area that database administrator's can configure to provide large memory allocations for the following:

- **User Global Area (UGA):** Session memory for the shared server and the Oracle XA interface (used where transactions interact with multiple databases)
- **I/O Buffer Area:** I/O server processes, message buffers used in parallel query operations, buffers for Recovery Manager (RMAN) I/O slaves, and advanced queuing memory table storage
- **Free memory**

The large pool is different from reserved space in the shared pool, which uses the same Least Recently Used (LRU) list as other memory allocated from the shared pool. The large pool does not have an LRU list. Pieces of memory are allocated and cannot be freed until they are done being used.

A request from a user is a single API call that is part of the user's SQL statement. In a dedicated server environment, one server process handles requests for a single client process. Each server process uses system resources, including CPU cycles and memory. In a shared server environment, the following actions occur:

1. A client application sends a request to the database instance, and that request is received by the dispatcher.
2. The dispatcher places the request on the request queue in the large pool.
3. The request is picked up by the next available shared server process. The shared server processes check the common request queue for new requests, picking up new requests on a first-in-first-out basis. One shared server process picks up one request in the queue.
4. The shared server process makes all the necessary calls to the database to complete the request. First, the shared server process accesses the library cache in the shared pool to verify the requested items; for example, it checks whether the table exists, whether the user has the correct privileges, and so on. Next, the shared server process accesses the buffer cache to retrieve the data. If the data is not there, the shared server process accesses the disk. A different shared server process can handle each database call. Therefore, requests to parse a query, fetch the first row, fetch the next row, and close the result set may each be processed by a different shared server process. Because a different shared server process may handle each database call, the User Global Area (UGA) must be a Shared Memory area, as the UGA contains information about each client session. Or reversed, the UGA contains information about each client session and must be available to all shared server processes because any shared server process may handle any session's database call.
5. After the request is completed, a shared server process places the response on the calling dispatcher's response queue in the large pool. Each dispatcher has its own response queue.
6. The response queue sends the response to the dispatcher.
7. The dispatcher returns the completed request to the appropriate client application.

For more information, see [Large Pool](http://www.oracle.com/pls/topic/lookup?ctx=db18&id=CNCPT1233).

![image-20201007230625369](../../../../../../AppData/Roaming/Typora/typora-user-images/image-20201007230625369.png)

The database buffer cache, also called the buffer cache, is the memory area in the System Global Area (SGA) that stores copies of data blocks read from data files. A buffer is a database block-sized chunk of memory. Each buffer has an address called a Database Buffer Address (DBA). All users concurrently connected to a database instance share access to the buffer cache. The goals of the buffer cache is to optimize physical I/O and to keep frequently accessed blocks in the buffer cache and write infrequently accessed blocks to disk.

The first time an Oracle Database user process requires a particular piece of data, it searches for the data in the database buffer cache. If the process finds the data already in the cache (a cache hit), it can read the data directly from memory. If the process cannot find the data in the cache (a cache miss), it must copy the data block from a data file on disk into a buffer in the cache before accessing the data. Accessing data through a cache hit is faster than accessing data through a cache miss.

The buffers in the cache are managed by a complex algorithm that uses a combination of least recently used (LRU) lists and touch count. The LRU helps to ensure that the most recently used blocks tend to stay in memory to minimize disk access.

The database buffer cache consists of the following:

- **Default pool:** Is the location where blocks are normally cached. The default block size is 8 KB. Unless you manually configure separate pools, the default pool is the only buffer pool. The optional configuration of the other pools has no effect on the default pool.
- **Keep pool:** Is intended for blocks that were accessed frequently, but which aged out of the default pool because of lack of space. The purpose of the keep buffer pool is to retain specified objects in memory, thus avoiding I/O operations.
- **Recycle pool:** Is intended for blocks that are used infrequently. A recycle pool prevents specified objects from consuming unnecessary space in the cache.
- **Non-default buffer pools:** Are for tablespaces that use the nonstandard block sizes of 2 KB, 4 KB, 16 KB, and 32 KB. Each non-default block size has its own pool. Oracle Database manages the blocks in these pools in the same way as in the default pool.
- **Database Smart Flash Cache (flash cache):** Lets you use flash devices to increase the effective size of the buffer cache without adding more main memory. Flash cache can improve database performance by having the database cache's frequently accessed data stored into flash memory instead of reading the data from magnetic disk. When the database requests data, the system first looks in the database buffer cache. If the data is not found, the system then looks in the Database Smart Flash Cache buffer. If it does not find the data there, only then does it look in disk storage. You must configure a flash cache on either all or none of the instances in an Oracle Real Application Clusters environment.
- **Flash Buffer Area:** Consists of a DEFAULT Flash LRU Chain and a KEEP Flash LRU Chain. Without Database Smart Flash Cache, when a process tries to access a block and the block does not exist in the buffer cache, the block will first be read from disk into memory (physical read). When the in-memory buffer cache gets full, a buffer will get evicted out of the memory based on a least recently used (LRU) mechanism. With Database Smart Flash Cache, when a clean in-memory buffer is aged out, the buffer content is written to the flash cache in the background by the Database Writer process (DBWn), and the buffer header is kept in memory as metadata in either the DEFAULT flash or KEEP Flash LRU list, depending on the value of the FLASH_CACHE object attribute. The KEEP flash LRU list is used to maintain the buffer headers on a separate list to prevent the regular buffer headers from replacing them. Thus, the flash buffer headers belonging to an object specified as KEEP tend to stay in the flash cache longer. If the FLASH_CACHE object attribute is set to NONE, the system does not retain the corresponding buffers in the flash cache or in memory. When a buffer that was already aged out of memory is accessed again, the system checks the flash cache. If the buffer is found, it reads it back from the flash cache which takes only a fraction of the time of reading from the disk. The consistency of flash cache buffers across Real Application Clusters (RAC) is maintained in the same way as by Cache Fusion. Because the flash cache is an extended cache and direct path I/O totally bypasses the buffer cache, this feature does not support direct path I/O. Note that the system does not put dirty buffers in flash cache because it may have to read buffers into memory in order to checkpoint them because writing to flash cache does not count for checkpoint.

For more information, see [Database Buffer Cache](http://www.oracle.com/pls/topic/lookup?ctx=db18&id=CNCPT1222).

![image-20201007230653699](../../../../../../AppData/Roaming/Typora/typora-user-images/image-20201007230653699.png)

The In-Memory Area is an optional SGA component that contains the In-Memory column store (IM column store), which stores tables and partitions in memory using a columnar format optimized for rapid scans. The IM column store enables data to be simultaneously populated in the SGA in both the traditional row format (in the buffer cache) and a columnar format. The database transparently sends online transactional processing (OLTP) queries, such as primary key lookups, to the buffer cache, and analytic and reporting queries to the IM column store. When fetching data, Oracle Database can also read data from both memory areas within the same query. The dual-format architecture does not double memory requirements. The buffer cache is optimized to run with a much smaller size than the size of the database.

You should populate only the most performance-critical data in the IM column store. To add an object to the IM column store, turn on the INMEMORY attribute for an object when creating or altering it. You can specify this attribute on a tablespace (for all new tables and views in the tablespace), table, (sub)partition, materialized view, or subset of columns within an object.

The IM column store manages both data and metadata in optimized storage units, not in traditional Oracle data blocks. An In-Memory Compression Unit (IMCU) is a compressed, read-only storage unit that contains data for one or more columns. A Snapshot Metadata Unit (SMU) contains metadata and transactional information for an associated IMCU. Every IMCU maps to a separate SMU.

The Expression Statistics Store (ESS) is a repository that stores statistics about expression evaluation. The ESS resides in the SGA and also persists on disk. When an IM column store is enabled, the database leverages the ESS for its In-Memory Expressions (IM expressions) feature. An In-Memory Expression Unit (IMEU) is a storage container for materialized IM expressions and user-defined virtual columns. Note that the ESS is independent of the IM column store. The ESS is a permanent component of the database and cannot be disabled.

Conceptually, an IMEU is a logical extension of its parent IMCU. Just as an IMCU can contain multiple columns, an IMEU can contain multiple virtual columns. Every IMEU maps to exactly one IMCU, mapping to the same row set. The IMEU contains expression results for the data contained in its associated IMCU. When the IMCU is populated, the associated IMEU is also populated.

A typical IM expression involves one or more columns, possibly with constants, and has a one-to-one mapping with the rows in the table. For example, an IMCU for an EMPLOYEES table contains rows 1-1000 for the column weekly_salary. For the rows stored in this IMCU, the IMEU calculates the automatically detected IM expression weekly_salary*52, and the user-defined virtual column quarterly_salary defined as weekly_salary*12. The third row down in the IMCU maps to the third row down in the IMEU.

The In-Memory area is sub-divided into two pools: a 1MB columnar data pool used to store the actual column-formatted data populated into memory (IMCUs and IMEUs), and a 64K metadata pool used to store metadata about the objects that are populated into the IM column store. The relative size of the two pools is determined by internal heuristics; the majority of the In-Memory area memory is allocated to the 1MB pool. The size of the In-Memory area is controlled by the initialization parameter INMEMORY_SIZE (default 0) and must have a minimum size of 100MB. Starting in Oracle Database 12.2, you can increase the size of the In-Memory area on the fly by increasing the INMEMORY_SIZE parameter via an ALTER SYSTEM command by at least 128MB. Note that it is not possible to shrink the size of the In-Memory area on the fly.

An in-memory table gets IMCUs allocated in the IM column store at first table data access or at database startup. An in-memory copy of the table is made by doing a conversion from the on-disk format to the new in-memory columnar format. This conversion is done each time the instance restarts as the IM column store copy resides only in memory. When this conversion is done, the in-memory version of the table gradually becomes available for queries. If a table is partially converted, queries are able to use the partial in-memory version and go to disk for the rest, rather than waiting for the entire table to be converted.

In response to queries and data manipulation language (DML), server processes scan columnar data and update SMU metadata. Background processes populate row data from disk into the IM column store. The In-Memory Coordinator Process (IMCO) is a background process that initiates background population and repopulation of columnar data. The Space Management Coordinator Process (SMCO) and Space Management Worker Processes (Wnnn) are background processes that do the actual populating and repopulating of data on behalf of IMCO. DML block changes are written to the buffer cache, and then to disk. Background processes then repopulate row data from disk into the IM column store based on the metadata invalidations and query requests.

You can enable the In-Memory FastStart feature to write the columnar data in the IM Column Store back to a tablespace in the database in compressed columnar format. This feature makes database startup faster. Note that this feature does not apply to IMEUs. They are always populated dynamically from the IMCUs.

For more information, see [Introduction to Oracle Database In-Memory](http://www.oracle.com/pls/topic/lookup?ctx=db18&id=GUID-BFA53515-7643-41E5-A296-654AB4A9F9E7).

- [Facebook](https://www.facebook.com/Oracle/)

![image-20201007230735091](../../../../../../AppData/Roaming/Typora/typora-user-images/image-20201007230735091.png)

A database is a set of physical files that store user data and metadata. The metadata consists of structural, configuration, and control information about the database server. You can design your database to be a multitenant container database (CDB) or a non-container database (non-CDB).

A CDB is made up of one CDB root container (also called the root), exactly one seed pluggable database (seed PDB), zero or more user-created pluggable databases (simply referred to as PDBs), and zero or more application containers. The entire CDB is referred to as the system container. To a user or application, PDBs appear logically as separate databases.

The CDB root, named CDB$ROOT, contains multiple data files, control files, redo log files, flashback logs, and archived redo log files. The data files store Oracle-supplied metadata and common users (users that are known in every container), which are shared with all PDBs.

The seed PDB, named PDB$SEED, is a system-supplied PDB template containing multiple data files that you can use to create new PDBs.

The regular PDB contains multiple data files that contain the data and code required to support an application; for example, a Human Resources application. Users interact only with the PDBs, and not the seed PDB or root container. You can create multiple PDBs in a CDB. One of the goals of the multitenant architecture is that each PDB has a one-to-one relationship with an application.

An application container is an optional collection of PDBs within a CDB that stores data for an application. The purpose of creating an application container is to have a single master application definition. You can have multiple application containers in a CDB.

A database is divided into logical storage units called tablespaces, which collectively store all the database data. Each tablespace represents one or more data files. The root container and regular PDBs have a SYSTEM, SYSAUX, USERS, TEMP, and UNDO tablespace (optional in a regular PDB). A seed PDB has a SYSTEM, SYSAUX, TEMP, and optional UNDO tablespace.

For more information, see [Introduction to the Multitenant Architecture](http://www.oracle.com/pls/topic/lookup?ctx=db18&id=CNCPT89234).

![image-20201007230803642](../../../../../../AppData/Roaming/Typora/typora-user-images/image-20201007230803642.png)

The following database system files are used during the operation of an Oracle Database and reside on the database server. Note that data files are physical files that belong to database containers and are not described here.

- **Control files:** A control file is a required file that stores metadata about the data files and online redo log files; for example, their names and statuses. This information is required by the database instance to open the database. Control files also contain metadata that must be accessible when the database is not open. It is highly recommended that you make several copies of the control file in your database server for high availability.
- **Parameter file:** This required file defines how the database instance is configured when it starts up. It can be either an initialization parameter file (pfile) or a server parameter file (spfile).
- **Online redo log files:** These required files store changes to the database as they occur and are used for data recovery.
- **Automatic Diagnostic Repository (ADR):** The ADR is a file-based repository for database diagnostic data, such as traces, dumps, the alert log, health monitor reports, and more. It has a unified directory structure across multiple instances and multiple products. The database, Oracle Automatic Storage Management (Oracle ASM), the listener, Oracle Clusterware, and other Oracle products or components store all diagnostic data in the ADR. Each instance of each product stores diagnostic data underneath its own home directory within the ADR.
- **Backup files:** These optional files are used for database recovery. You typically restore a backup file when a media failure or user error has damaged or deleted the original file.
- **Archived redo log files:** These optional files contain an ongoing history of the data changes that are generated by the database instance. Using these files and a backup of the database, you can recover a lost data file. That is, archive logs enable the recovery of restored data files.
- **Password file:** This optional file enables users using the SYSDBA, SYSOPER, SYSBACKUP, SYSDG, SYSKM, SYSRAC, and SYSASM roles to connect remotely to the database instance and perform administrative tasks.
- **Wallets:** For large-scale deployments where applications use password credentials to connect to databases, it is possible to store such credentials in a client-side Oracle wallet. An Oracle wallet is a secure software container that is used to store authentication and signing credentials. Possible wallets include an Oracle wallet for user credentials, Encryption Wallet for Transparent Data Encryption (TDE), and an Oracle Public Cloud (OPC) wallet for the database backup cloud module. A wallet is optional, but recommended.
- **Block change tracking file:** Block change tracking improves the performance of incremental backups by recording changed blocks in the block change tracking file. During an incremental backup, instead of scanning all data blocks to identify which blocks have changed, Oracle Recovery Manager (RMAN) uses this file to identify the changed blocks that need to be backed up. A block change tracking file is optional.
- **Flashback logs:** Flashback Database is similar to conventional point-in-time recovery in its effects. It enables you to return a database to its state at a time in the recent past. Flashback Database uses its own logging mechanism, creating flashback logs and storing them in the fast recovery area. You can use Flashback Database only if flashback logs are available. To take advantage of this feature, you must set up your database in advance to create flashback logs. Flashback logs are optional.

Control files, online redo log files, and archive redo log files can be multiplexed, which mean that two or more identical copies can be automatically maintained in separate locations.

The control file, parameter file, and online redo log files are required for database startup. For more information, see [Physical Storage Structures](http://www.oracle.com/pls/topic/lookup?ctx=db18&id=CNCPT003).

![image-20201007230830837](../../../../../../AppData/Roaming/Typora/typora-user-images/image-20201007230830837.png)

An application container is an optional, user-created CDB component that stores data and metadata for application PDBs. A CDB can include zero or more application containers. An application container consists of exactly one application root and one or more application PDBs, which plug into the CDB root. An application root belongs to the CDB root and no other container, and stores the common metadata and data.

A typical application installs application common users, metadata-linked common objects, and data-linked common objects. You might create multiple sales-related PDBs within one application container, with these PDBs sharing an application back end that consists of a set of common tables and table definitions.

The application root, application seed, and application PDB each have a SYSTEM, SYSAUX, TEMP, USERS, and optional UNDO tablespace. Each tablespace represents one or more data files.

For more information, see [About Application Containers](http://www.oracle.com/pls/topic/lookup?ctx=db18&id=GUID-8F8979E3-9B81-4293-B3CF-ACCF42A0B2D6).

![image-20201007230900697](../../../../../../AppData/Roaming/Typora/typora-user-images/image-20201007230900697.png)

The Automatic Diagnostic Repository (ADR) is a system-wide tracing and logging central repository for database diagnostic data. It includes the following items:

- **Background trace files:** Each database background process can write to an associated trace file. When a process detects an internal error, the process dumps information about the error to its trace file. Some of the information written to a trace file is intended for the database administrator, whereas other information is for Oracle Support Services. Typically, database background process trace file names contain the Oracle system identifier (SID), the background process name, and the operating system process number. An example of a trace file for the RECO process is mytest_reco_10355.trc.
- **Foreground trace files:** Each server process can write to an associated trace file. When a process detects an internal error, the process dumps information about the error to its trace file. Server process trace file names contain the Oracle SID, the string ora, and the operating system process number. An example of a server process trace file name is mytest_ora_10304.trc.
- **Dump files:** A diagnostic dump file is a special type of trace file that contains detailed point-in-time information about a state or structure. A dump file is typically a one-time output of diagnostic data in response to an event whereas a trace file tends to be a continuous output of diagnostic data.
- **Health monitor reports:** Oracle Database includes a framework called Health Monitor for running diagnostic checks on the database. Health checks detect file corruptions, physical and logical block corruptions, undo and redo corruptions, data dictionary corruptions, and more. The health checks generate reports of their findings and, in many cases, recommendations for resolving problems.
- **Incident packages:** For the customized approach to uploading diagnostic data to Oracle Support, you first collect the data into an intermediate logical structure called an incident package (package). A package is a collection of metadata that is stored in the ADR and points to diagnostic data files and other files both in and outside of the ADR. When you create a package, you select one or more problems to add to the package. The Support Workbench then automatically adds to the package the problem information, incident information, and diagnostic data (such as trace files and dumps) associated with the selected problems.
- **Incident dumps:** When an incident occurs, the database writes one or more dumps to the incident directory created for the incident. Incident dumps also contain the incident number in the file name.
- **Alert log file:** The alert log of a database is an chronological log of messages and errors. Oracle recommends that you review the alert log periodically.

![image-20201007230932884](../../../../../../AppData/Roaming/Typora/typora-user-images/image-20201007230932884.png)

Database backups can be either physical or logical.

- **Physical backups** are copies of physical database files. You can make physical backups with Recovery Manager (RMAN) or operating system utilities.
- **Logical backups** contain tables, stored procedures, and other logical data. You can extract logical data with an Oracle Database utility, such as Data Pump Export, and store it in a binary file. Logical backups can supplement physical backups.

Database backups created by RMAN are stored as image copies or backup sets.

- An **image copy** is a bit-for-bit, on-disk duplicate of a data file, control file, or archived redo log file. You can create image copies of physical files with operating system utilities or RMAN and use either tool to restore them. Image copies are useful for disk because you can update them incrementally and recover them in place.
- A **backup set** is a proprietary format created by RMAN that contains the data from one or more data files, archived redo log files, control files, or server parameter file. The smallest unit of a backup set is a binary file called a backup piece. Backup sets are the only form in which RMAN can write backups to sequential devices, such as tape drives. One advantage of backup sets is that RMAN uses unused block compression to save space in backing up data files. Only those blocks in the data files that have been used to store data are included in the backup set. Backup sets can also be compressed, encrypted, sent to tape, and use advanced unused-space compression that is not available with datafile copies.

RMAN can interface with Media Management Library (MML) or System Backup to Tape (SBT) software, which can create backups to tape, Oracle Database Backup Cloud Service, or Zero Data Loss Recovery Appliance (commonly known as Recovery Appliance).

For more information, see:

- [Backup and Recovery](http://www.oracle.com/pls/topic/lookup?ctx=db18&id=CNCPT031)
- [About Zero Data Loss Recovery Appliance](http://www.oracle.com/pls/topic/lookup?ctx=db18&id=GUID-25997EE3-A3E0-4ED2-AA63-3B4067F3BDF3)

![image-20201007231000242](../../../../../../AppData/Roaming/Typora/typora-user-images/image-20201007231000242.png)

rocess Monitor Process (PMON) is a background process that periodically scans all processes to find any that have died abnormally. PMON is then responsible for coordinating cleanup performed by the Cleanup Main Process (CLMN) and the Cleanup Slave Process slaves (CLnn).

PMON runs as an operating system process, and not as a thread. In addition to database instances, PMON also runs on Oracle Automatic Storage Management (ASM) instances and Oracle ASM Proxy instances.

For a complete list of background processes, see [Background Processes](http://www.oracle.com/pls/topic/lookup?ctx=db18&id=REFRN104).

![image-20201007231020824](../../../../../../AppData/Roaming/Typora/typora-user-images/image-20201007231020824.png)

Process Manager Process (PMAN) is a background process that monitors, spawns, and stops the following as needed:

- Dispatcher and shared server processes
- Connection broker and pooled server processes for database resident connection pools
- Job queue processes
- Restartable background processes

PMAN runs as an operating system process, and not as a thread. In addition to database instances, PMAN also runs on Oracle Automatic Storage Management (ASM) instances and Oracle ASM Proxy instances.

For a complete list of background processes, see [Background Processes](http://www.oracle.com/pls/topic/lookup?ctx=db18&id=REFRN104).

![image-20201007231047501](../../../../../../AppData/Roaming/Typora/typora-user-images/image-20201007231047501.png)

Listener Registration Process (LREG) is a background process that notifies the listeners about instances, services, handlers, and endpoints.

LREG can run as a thread or an operating system process. In addition to database instances, LREG also runs on Oracle Automatic Storage Management (ASM) instances and Oracle Real Application Clusters (RAC).

For a complete list of background processes, see [Background Processes](http://www.oracle.com/pls/topic/lookup?ctx=db18&id=REFRN104).

![image-20201007231108151](../../../../../../AppData/Roaming/Typora/typora-user-images/image-20201007231108151.png)

System Monitor Process (SMON) is a background process that performs many database maintenance tasks, including the following:

- Creates and manages the temporary tablespace metadata, and reclaims space used by orphaned temporary segments
- Maintains the undo tablespace by onlining, offlining, and shrinking the undo segments based on undo space usage statistics
- Cleans up the data dictionary when it is in a transient and inconsistent state
- Maintains the System Change Number (SCN) to time mapping table used to support Oracle Flashback features

SMON is resilient to internal and external errors raised during background activities. SMON can run as a thread or an operating system process. In an Oracle Real Application Clusters (RAC) database, the SMON process of one instance can perform instance recovery for other instances that have failed.

For a complete list of background processes, see [Background Processes](http://www.oracle.com/pls/topic/lookup?ctx=db18&id=REFRN104).

![image-20201007231129734](../../../../../../AppData/Roaming/Typora/typora-user-images/image-20201007231129734.png)

Database Writer Process (DBWn) is a background process that primarily writes data blocks to disk. It also handles checkpoints, file open synchronization, and logging of Block Written records. DBWn also writes to the Database Smart Flash Cache (Flash Cache), when Flash Cache is configured.

In many cases the blocks that DBWn writes are scattered throughout the disk. Thus, the writes tend to be slower than the sequential writes performed by the Log Writer Process (LGWR). DBWn performs multi-block writes when possible to improve efficiency. The number of blocks written in a multi-block write varies by operating system.

The DB_WRITER_PROCESSES initialization parameter specifies the number of Database Writer Processes. There can be 1 to 100 Database Writer Processes. The names of the first 36 Database Writer Processes are DBW0-DBW9 and DBWa-DBWz. The names of the 37th through 100th Database Writer Processes are BW36-BW99. The database selects an appropriate default setting for the DB_WRITER_PROCESSES parameter or adjusts a user-specified setting based on the number of CPUs and processor groups.

For a complete list of background processes, see [Background Processes](http://www.oracle.com/pls/topic/lookup?ctx=db18&id=REFRN104).

![image-20201007231154178](../../../../../../AppData/Roaming/Typora/typora-user-images/image-20201007231154178.png)

Checkpoint Process (CKPT) is a background process that, at specific times, starts a checkpoint request by messaging Database Writer Process (DBWn) to begin writing dirty buffers. On completion of individual checkpoint requests, CKPT updates data file headers and control files to record the most recent checkpoint.

CKPT checks every three seconds to see whether the amount of memory exceeds the value of the PGA_AGGREGATE_LIMIT initialization parameter, and if so, takes action.

CKPT can run as a thread or an operating system process. In addition to database instances, CKPT also runs on Oracle Automatic Storage Management (ASM) instances.

For a complete list of background processes, see [Background Processes](http://www.oracle.com/pls/topic/lookup?ctx=db18&id=REFRN104).

![image-20201007231301751](../../../../../../AppData/Roaming/Typora/typora-user-images/image-20201007231301751.png)

Manageability Monitor Process (MMON) and Manageability Monitor Lite Process (MMNL) are background processes that perform tasks related to the Automatic Workload Repository (AWR). The AWR is a repository of historical performance data that includes cumulative statistics for the system, sessions, individual SQL statements, segments, and services. It is used for problem detection and self-tuning purposes.

MMON gathers a variety of memory statistics from the SGA, filters them, and then creates snapshots of those statistics every 60 minutes in the Automatic Workload Repository (AWR). It also performs Automatic Database Diagnostic Monitor (ADDM) analysis and issues alerts for metrics that exceed their threshold values.

MMNL gathers session statistics (such as the user ID, state, the machine, and the SQL it is executing) and stores them in the Active Session History (ASH) buffer. Specifically, MMNL samples the V$SESSION and V$SESSION_WAIT views every second in the SGA and then records that data in the V$ACTIVE_SESSION_HISTORY view. Inactive sessions are not sampled. The ASH is designed as a rolling buffer in memory, and therefore, earlier information is overwritten when needed. When the ASH buffer becomes full or when MMON takes a snapshot, MMNL flushes (empties) the ASH buffer into the DBA_HIST_ACTIVE_SESS_HISTORY view in the AWR. Because space is expensive, only one in every 10 entries is flushed. MMNL also computes metrics.

Both MMON and MMNL can run as threads or as an operating system processes. In addition to database instances, MMON and MMNL also run on Automatic Storage Management (ASM) instances.

For more information, see:

- [Managing the SYSAUX Tablespace](http://www.oracle.com/pls/topic/lookup?ctx=db18&id=ADMIN11384)
- [Managing the Automatic Workload Repository](http://www.oracle.com/pls/topic/lookup?ctx=db18&id=TGDBA185)
- [Active Session History Statistics](http://www.oracle.com/pls/topic/lookup?ctx=db18&id=GUID-270E36D0-D7D7-4235-8280-CA3EDD68F8E6)

For a complete list of background processes, see [Background Processes](http://www.oracle.com/pls/topic/lookup?ctx=db18&id=REFRN104).

![image-20201007231326242](../../../../../../AppData/Roaming/Typora/typora-user-images/image-20201007231326242.png)

Recoverer Process (RECO) is a background process that resolves distributed transactions that are pending because of a network or system failure in a distributed database.

RECO can run as a thread or as an operating system process.

For a complete list of background processes, see [Background Processes](http://www.oracle.com/pls/topic/lookup?ctx=db18&id=REFRN104).

![image-20201007231348152](../../../../../../AppData/Roaming/Typora/typora-user-images/image-20201007231348152.png)

Log Writer Process (LGWR) is a background process that writes redo log entries sequentially into a redo log file. Redo log entries are generated in the redo log buffer of the System Global Area (SGA). If the database has a multiplexed redo log, then LGWR writes the same redo log entries to all of the members of a redo log file group.

LGWR handles the operations that are very fast, or must be coordinated, and delegates operations to the Log Writer Worker helper processes (LGnn) that could benefit from concurrent operations, primarily writing the redo from the log buffer to the redo log file and posting the completed write to the foreground process that is waiting.

The Redo Transport Slave Process (TT00-zz) ships redo from the current online and standby redo logs to remote standby destinations configured for Asynchronous (ASYNC) redo transport.

LGWR can run as a thread or as an operating system process. In addition to database instances, LGWR also runs on Oracle ASM instances. Each database instance in an Oracle Real Application Clusters (RAC) configuration has its own set of redo log files.

For a complete list of background processes, see [Background Processes](http://www.oracle.com/pls/topic/lookup?ctx=db18&id=REFRN104).

![image-20201007231411043](../../../../../../AppData/Roaming/Typora/typora-user-images/image-20201007231411043.png)

Archiver Processes (ARCn) are background processes that exist only when the database is in ARCHIVELOG mode and automatic archiving is enabled, in which case ARCn automatically archives online redo log files. Log Writer Process (LGWR) cannot reuse and overwrite an online redo log group until it has been archived.

The database starts multiple archiver processes as needed to ensure that the archiving of filled online redo logs does not fall behind. Possible processes include ARC0-ARC9 and ARCa-ARCt (31 possible destinations).

The LOG_ARCHIVE_MAX_PROCESSES initialization parameter specifies the number of ARCn processes that the database initially invokes. If you anticipate a heavy workload for archiving, such as during bulk loading of data, you can increase the maximum number of archiver processes. There can also be multiple archive log destinations. It is recommended that there be at least one archiver process for each destination.

ARCn can run as a thread or as an operating system process.

For a complete list of background processes, see [Background Processes](http://www.oracle.com/pls/topic/lookup?ctx=db18&id=REFRN104).

![image-20201007231435479](../../../../../../AppData/Roaming/Typora/typora-user-images/image-20201007231435479.png)

Job Queue Coordinator Process (CJQ0) is a background process that selects jobs that need to be run from the data dictionary and spawns Job Queue Slave Processes (Jnnn) to run the jobs. CJQ0 is automatically started and stopped as needed by Oracle Scheduler. The JOB_QUEUE_PROCESSES initialization parameter specifies the maximum number of processes that can be created for the execution of jobs. CJQ0 starts only as many job queue processes as required by the number of jobs to run and available resources.

A Job Queue Slave Process (Jnnn) executes jobs assigned by the job coordinator. When a job is picked for processing, the job slave does the following:

- Gathers all the metadata needed to run the job, for example, program arguments and privilege information.
- Starts a database session as the owner of the job, starts a transaction, and then starts executing the job.
- Once the job is complete, the slave commits and ends the transaction.
- Closes the session.

When a job is done, the slaves do the following:

- Reschedule the job if required
- Update the state in the job table to reflect whether the job has completed or is scheduled to run again
- Insert an entry into the job log table
- Update the run count, and if necessary, failure and retry counts
- Clean up
- Look for new work (if none, they go to sleep)

Both CJQ0 and Jnnn can run as threads or as operating system processes.

For a complete list of background processes, see [Background Processes](http://www.oracle.com/pls/topic/lookup?ctx=db18&id=REFRN104).

![image-20201007231458839](../../../../../../AppData/Roaming/Typora/typora-user-images/image-20201007231458839.png)

Recovery Writer Process (RVWR) is a background process that is used to flashback an entire database. That is, it undoes transactions from the current state of the database to a time in the past, provided you have the required flashback logs. When flashback is enabled or when there are guaranteed restore points, RVWR writes flashback data to flashback database logs in the fast recovery area.

RVWR can run as a thread or as an operating system process.

For a complete list of background processes, see [Background Processes](http://www.oracle.com/pls/topic/lookup?ctx=db18&id=REFRN104).

![image-20201007231521387](../../../../../../AppData/Roaming/Typora/typora-user-images/image-20201007231521387.png)

Flashback Data Archiver Process (FBDA) is a background process that provides the ability to track and store transactional changes to a table over its lifetime. This way, you can flashback tables back in time to restore the way they were.

When a transaction that modifies a tracked table commits, FBDA stores the pre-image of the rows in the archive. FBDA maintains metadata on the current rows and tracks how much data has been archived.

FBDA is also responsible for automatically managing the flashback data archive for space, organization (partitioning tablespaces), and retention. FBDA also keeps track of how far the archiving of tracked transactions has progressed.

FBDA can run as a thread or as an operating system process.

For a complete list of background processes, see [Background Processes](http://www.oracle.com/pls/topic/lookup?ctx=db18&id=REFRN104).

![image-20201007231547140](../../../../../../AppData/Roaming/Typora/typora-user-images/image-20201007231547140.png)

Space Management Coordinator Process (SMCO) is a background process that schedules the execution of various space management tasks, including proactive space allocation and space reclamation. SMCO dynamically spawns Space Management Slave Processes (Wnnn) to implement these tasks. Note that the In-Memory Coordinator Process (IMCO) is a background process that initiates background population and repopulation of columnar data.

Wnnn slave processes perform work on behalf of Space Management and on behalf of the Oracle In-Memory option.

- Wnnn processes are slave processes dynamically spawned by SMCO to perform space management tasks in the background. These tasks include preallocating space into locally managed tablespace and SecureFiles segments based on space usage growth analysis, and reclaiming space from dropped segments. After being started, the slave acts as an autonomous agent. After it finishes task execution, it automatically picks up another task from the queue. The process terminates itself after being idle for a long time.
- Wnnn processes populate and repopulate in-memory enabled objects. The In-Memory Coordinator Process (IMCO) initiates background population and repopulation of columnar data. The IMCO background process and foreground processes will utilize Wnnn slaves for population and repopulation. Wnnn processes are utilized by IMCO for prepopulation of in-memory enabled objects with priority LOW/MEDIUM/HIGH/CRITICAL, and for repopulation of in-memory objects. In-memory populate and repopulate tasks running on Wnnn slaves are also initiated from foreground processes in response to queries and DMLs that reference in-memory enabled objects.

Both SMCO and Wnnn can run as threads or as operating system processes.

For a complete list of background processes, see [Background Processes](http://www.oracle.com/pls/topic/lookup?ctx=db18&id=REFRN104).

![image-20201007231619515](../../../../../../AppData/Roaming/Typora/typora-user-images/image-20201007231619515.png)

In a shared server architecture, clients connect to a Dispatcher Process (Dnnn), which creates a virtual circuit for each connection. When the client sends data to the server, the dispatcher receives the data into the virtual circuit and places the active circuit on the common queue to be picked up by an idle Shared Server process (Snnn). The Snnn then reads the data from the virtual circuit and performs the database work necessary to complete the request. When the Snnn must send data to the client, the Snnn writes the data back into the virtual circuit and the Dnnn sends the data to the client. After the Snnn completes the client request, it releases the virtual circuit back to the Dnnn and is free to handle other clients.

Both Snnn and Dnnn can run as threads or as operating system processes. In addition to database instances, Dnnn also runs on shared servers.

For information about how Dnnn and Snnn interact with the Large Pool, see the [Large Pool](https://www.oracle.com/webfolder/technetwork/tutorials/architecture-diagrams/18/technical-architecture/database-technical-architecture.html#) slide. For a complete list of background processes, see [Background Processes](http://www.oracle.com/pls/topic/lookup?ctx=db18&id=REFRN104).